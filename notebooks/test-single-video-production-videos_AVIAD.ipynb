{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cfeb927e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:40:04.526828Z",
     "start_time": "2023-05-26T09:40:01.420220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'loqui'\n",
      "/tf/loqui\n",
      "Already on 'main'\r\n",
      "Your branch is up to date with 'origin/main'.\r\n",
      "From https://github.com/aviadshimoni/loqui\r\n",
      " * branch            main       -> FETCH_HEAD\r\n",
      "Already up to date.\r\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (2.0.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (1.23.5)\r\n",
      "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (4.7.0.72)\r\n",
      "Requirement already satisfied: PyTurboJPEG in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (1.7.1)\r\n",
      "Requirement already satisfied: matplotlib~=3.7.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (3.7.1)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (3.1.2)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (3.12.0)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (8.5.0.96)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (11.7.99)\r\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (10.2.10.91)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (11.7.99)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (11.7.91)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (11.10.3.66)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (11.7.4.91)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (2.14.3)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (1.11.1)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (3.1)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (11.7.101)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (10.9.0.58)\r\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (2.0.0)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (11.4.0.1)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.3.0->-r requirements.txt (line 1)) (4.5.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3.0->-r requirements.txt (line 1)) (67.6.0)\r\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.3.0->-r requirements.txt (line 1)) (0.40.0)\r\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch>=1.3.0->-r requirements.txt (line 1)) (16.0.2)\r\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch>=1.3.0->-r requirements.txt (line 1)) (3.26.3)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.7.1->-r requirements.txt (line 5)) (1.4.4)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.7.1->-r requirements.txt (line 5)) (4.39.2)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.7.1->-r requirements.txt (line 5)) (9.4.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.7.1->-r requirements.txt (line 5)) (23.0)\r\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.7.1->-r requirements.txt (line 5)) (5.12.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.7.1->-r requirements.txt (line 5)) (2.8.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.7.1->-r requirements.txt (line 5)) (0.11.0)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.7.1->-r requirements.txt (line 5)) (1.0.7)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib~=3.7.1->-r requirements.txt (line 5)) (3.0.9)\r\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=3.2.0->matplotlib~=3.7.1->-r requirements.txt (line 5)) (3.15.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib~=3.7.1->-r requirements.txt (line 5)) (1.14.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.3.0->-r requirements.txt (line 1)) (2.1.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.3.0->-r requirements.txt (line 1)) (1.3.0)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "%cd loqui\n",
    "! git checkout main && git pull origin main\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "42aa6153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:40:08.895255Z",
     "start_time": "2023-05-26T09:40:04.530571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch >= 1.3.0\r\n",
      "numpy >= 1.16.4\r\n",
      "opencv-python >= 4.1.0\r\n",
      "PyTurboJPEG\r\n",
      "matplotlib~=3.7.1Requirement already satisfied: face_alignment in /usr/local/lib/python3.8/dist-packages (1.3.5)\r\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from face_alignment) (4.7.0.72)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from face_alignment) (2.0.0)\r\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (from face_alignment) (0.20.0)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from face_alignment) (4.65.0)\r\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from face_alignment) (0.57.0)\r\n",
      "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.8/dist-packages (from face_alignment) (1.9.1)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from face_alignment) (1.23.5)\r\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->face_alignment) (0.40.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba->face_alignment) (6.1.0)\r\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->face_alignment) (0.2)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->face_alignment) (1.4.1)\r\n",
      "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->face_alignment) (2.29.0)\r\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image->face_alignment) (2023.4.12)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->face_alignment) (23.0)\r\n",
      "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.8/dist-packages (from scikit-image->face_alignment) (3.1)\r\n",
      "Requirement already satisfied: pillow>=9.0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->face_alignment) (9.4.0)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (11.10.3.66)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (10.9.0.58)\r\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (10.2.10.91)\r\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (2.0.0)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (3.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (11.7.4.91)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (11.7.99)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (11.4.0.1)\r\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (1.11.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (2.14.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (11.7.91)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (11.7.101)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (4.5.0)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (8.5.0.96)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (11.7.99)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from torch->face_alignment) (3.12.0)\r\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->face_alignment) (0.40.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch->face_alignment) (67.6.0)\r\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch->face_alignment) (3.26.3)\r\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.8/dist-packages (from triton==2.0.0->torch->face_alignment) (16.0.2)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba->face_alignment) (3.15.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch->face_alignment) (2.1.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->torch->face_alignment) (1.3.0)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\r\n",
      "Requirement already satisfied: dlib in /usr/local/lib/python3.8/dist-packages (19.24.1)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!cat requirements.txt\n",
    "!pip install face_alignment\n",
    "!pip install dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "9b990ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:40:08.901354Z",
     "start_time": "2023-05-26T09:40:08.897917Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "db7fd884",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:40:08.929192Z",
     "start_time": "2023-05-26T09:40:08.915008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n",
      "(256, 256)\n"
     ]
    }
   ],
   "source": [
    "def get_frame_sizes(file_name: str) -> list:\n",
    "    \"\"\"\n",
    "    Gets a path to an MP4 video file and returns the size (dimensions) of each frame.\n",
    "    :param file_name: Path to the MP4 video file.\n",
    "    :return: List of frame sizes in the given video file.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(file_name)\n",
    "    frame_sizes = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame_sizes.append(frame.shape[:2])  # Get width and height of the frame\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return frame_sizes\n",
    "\n",
    "file_name = \"/tf/single-videos/FRENCH_00002.mp4\"\n",
    "sizes = get_frame_sizes(file_name)\n",
    "for size in sizes:\n",
    "    print(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5ee241b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:40:09.033576Z",
     "start_time": "2023-05-26T09:40:08.929731Z"
    }
   },
   "outputs": [],
   "source": [
    "from turbojpeg import TurboJPEG\n",
    "import math\n",
    "jpeg = TurboJPEG()\n",
    "\n",
    "\n",
    "def extract_opencv(file_name: str) -> list:\n",
    "    \"\"\"\n",
    "    Gets a path to a video file, resizes frames to 256x256, and tries to extract the ROI from it.\n",
    "    :param file_name: Path to the video file.\n",
    "    :return: ROI of the given video file.\n",
    "    \"\"\"\n",
    "\n",
    "    video = []\n",
    "    cap = cv2.VideoCapture(file_name)\n",
    "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"length\", length)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()  # BGR\n",
    "        if ret:\n",
    "            # resized_frame = cv2.resize(frame, (256, 256))\n",
    "            # roi = resized_frame[115:211, 79:175]\n",
    "\n",
    "            video.append(frame)\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "\n",
    "    return video\n",
    "\n",
    "# def extract_opencv(file_name: str) -> list:\n",
    "#     \"\"\"\n",
    "#     Gets a path to a video file and tries to extract the ROI from it.\n",
    "#     :param file_name: Path to the video file.\n",
    "#     :return: ROI of the given video file.\n",
    "#     \"\"\"\n",
    "#     detector = dlib.get_frontal_face_detector()\n",
    "#     predictor = dlib.shape_predictor(\"/tf/loqui/shape_predictor_68_face_landmarks.dat\")\n",
    "#     video = []\n",
    "#     cap = cv2.VideoCapture(file_name)\n",
    "\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()  # BGR\n",
    "#         if ret:\n",
    "#             gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#             faces = detector(gray)\n",
    "#             for face in faces:\n",
    "#                 landmarks = predictor(gray, face)\n",
    "#                 # Define the region of interest (ROI) based on facial landmarks\n",
    "#                 min_x = min(landmarks.part(i).x for i in range(0, 68))\n",
    "#                 max_x = max(landmarks.part(i).x for i in range(0, 68))\n",
    "#                 min_y = min(landmarks.part(i).y for i in range(0, 68))\n",
    "#                 max_y = max(landmarks.part(i).y for i in range(0, 68))\n",
    "                \n",
    "#                 # Adjust the ROI boundaries to make it smaller\n",
    "#                 offset = 26\n",
    "#                 min_x += offset - 10\n",
    "#                 max_x -= offset\n",
    "#                 min_y += offset\n",
    "#                 max_y -= offset\n",
    "                \n",
    "#                 roi = frame[min_y:max_y, min_x:max_x]\n",
    "#                 video.append(roi)\n",
    "#         else:\n",
    "#             break\n",
    "#     cap.release()\n",
    "\n",
    "#     return video\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "88b9d6b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:40:09.053682Z",
     "start_time": "2023-05-26T09:40:09.042671Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import face_alignment\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, Process, Queue\n",
    "import time\n",
    "import os\n",
    "\n",
    "def get_faces(frames):\n",
    "    fa = face_alignment.FaceAlignment(face_alignment.LandmarksType._2D, flip_input=False, device='cuda')\n",
    "    all_landmarks = []\n",
    "    for frame in frames:\n",
    "        points_list = fa.get_landmarks(frame)\n",
    "        all_landmarks.append(points_list)\n",
    "\n",
    "    return  all_landmarks\n",
    "\n",
    "\n",
    "def get_position(size, padding=0.25):\n",
    "\n",
    "    x = [0.000213256, 0.0752622, 0.18113, 0.29077, 0.393397, 0.586856, 0.689483, 0.799124,\n",
    "                    0.904991, 0.98004, 0.490127, 0.490127, 0.490127, 0.490127, 0.36688, 0.426036,\n",
    "                    0.490127, 0.554217, 0.613373, 0.121737, 0.187122, 0.265825, 0.334606, 0.260918,\n",
    "                    0.182743, 0.645647, 0.714428, 0.793132, 0.858516, 0.79751, 0.719335, 0.254149,\n",
    "                    0.340985, 0.428858, 0.490127, 0.551395, 0.639268, 0.726104, 0.642159, 0.556721,\n",
    "                    0.490127, 0.423532, 0.338094, 0.290379, 0.428096, 0.490127, 0.552157, 0.689874,\n",
    "                    0.553364, 0.490127, 0.42689]\n",
    "\n",
    "    y = [0.106454, 0.038915, 0.0187482, 0.0344891, 0.0773906, 0.0773906, 0.0344891,\n",
    "                    0.0187482, 0.038915, 0.106454, 0.203352, 0.307009, 0.409805, 0.515625, 0.587326,\n",
    "                    0.609345, 0.628106, 0.609345, 0.587326, 0.216423, 0.178758, 0.179852, 0.231733,\n",
    "                    0.245099, 0.244077, 0.231733, 0.179852, 0.178758, 0.216423, 0.244077, 0.245099,\n",
    "                    0.780233, 0.745405, 0.727388, 0.742578, 0.727388, 0.745405, 0.780233, 0.864805,\n",
    "                    0.902192, 0.909281, 0.902192, 0.864805, 0.784792, 0.778746, 0.785343, 0.778746,\n",
    "                    0.784792, 0.824182, 0.831803, 0.824182]\n",
    "\n",
    "    x, y = np.array(x), np.array(y)\n",
    "\n",
    "    x = (x + padding) / (2 * padding + 1)\n",
    "    y = (y + padding) / (2 * padding + 1)\n",
    "    x = x * size\n",
    "    y = y * size\n",
    "    return np.array(list(zip(x, y)))\n",
    "\n",
    "def cal_area(anno):\n",
    "    return (anno[:,0].max() - anno[:,0].min()) * (anno[:,1].max() - anno[:,1].min())\n",
    "\n",
    "\n",
    "def transformation_from_points(points1, points2):\n",
    "    points1 = points1.astype(np.float64)\n",
    "    points2 = points2.astype(np.float64)\n",
    "\n",
    "    c1 = np.mean(points1, axis=0)\n",
    "    c2 = np.mean(points2, axis=0)\n",
    "    points1 -= c1\n",
    "    points2 -= c2\n",
    "    s1 = np.std(points1)\n",
    "    s2 = np.std(points2)\n",
    "    points1 /= s1\n",
    "    points2 /= s2\n",
    "\n",
    "    U, S, Vt = np.linalg.svd(points1.T * points2)\n",
    "    R = (U * Vt).T\n",
    "    return np.vstack([np.hstack(((s2 / s1) * R,\n",
    "                                       c2.T - (s2 / s1) * R * c1.T)),\n",
    "                         np.matrix([0., 0., 1.])])\n",
    "\n",
    "\n",
    "\n",
    "def anno_img(images, annos):\n",
    "\n",
    "    shapes = []\n",
    "    for i in range(len(images)):\n",
    "        img = images[i]\n",
    "        anno = annos[i]\n",
    "\n",
    "\n",
    "        count = 0\n",
    "        #\n",
    "        # with open(anno, 'r') as f:\n",
    "        #     # annos = [line.strip().split('\\t') for line in f.readlines()]\n",
    "        #     if(len(annos) == 0): return\n",
    "        #     for (i, anno) in enumerate(annos):\n",
    "        #         x, y = [], []\n",
    "        #         for p in anno:\n",
    "        #             _, __ = p[1:-1].split(',')\n",
    "        #             _, __ = float(_), float(__)\n",
    "        #             x.append(_)\n",
    "        #             y.append(__)\n",
    "        #         annos[i] = np.stack([x, y], 1)\n",
    "        #\n",
    "        anno = sorted(anno, key = cal_area, reverse=True)[0]\n",
    "        shape = []\n",
    "\n",
    "        shapes.append(anno[17:])\n",
    "\n",
    "\n",
    "    front256 = get_position(256)\n",
    "    M_prev = None\n",
    "    frames=[]\n",
    "\n",
    "    for (shape, img) in zip(shapes, images):\n",
    "        M = transformation_from_points(np.matrix(shape), np.matrix(front256))\n",
    "        img = cv2.warpAffine(img, M[:2], (256, 256))\n",
    "        (x, y) = front256[-20:].mean(0).astype(np.int32)\n",
    "        w = 160//2\n",
    "        img = img[y-w//2:y+w//2,x-w:x+w,...]\n",
    "        #cv2.imwrite(os.path.join(save_dir, file), img)\n",
    "        frames.append(img)\n",
    "\n",
    "    return np.array(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b43c4b1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:40:09.064624Z",
     "start_time": "2023-05-26T09:40:09.055012Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_frames(frames):\n",
    "    # Convert frames to numpy arrays\n",
    "    frames = [np.array(frame, dtype=np.uint8) for frame in frames]\n",
    "\n",
    "    # Resize the frames\n",
    "    # resized_frames = [cv2.resize(frame, input_shape) for frame in frames]\n",
    "    p = get_faces(frames)\n",
    "    frames = anno_img(frames, p)\n",
    "\n",
    "    # Resize the frames\n",
    "    resized_frames = [cv2.resize(frame, input_shape) for frame in frames]\n",
    "\n",
    "    # plot_frames(frames)\n",
    "    # resized_frames = [frame[p] for frame in frames]\n",
    "\n",
    "    # Convert frames to grayscale\n",
    "    grayscale_frames = [cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) for frame in resized_frames]\n",
    "\n",
    "    # Normalize the frames\n",
    "    normalized_frames = [(frame / 255.0).astype(np.float32) for frame in grayscale_frames]\n",
    "    # tensor_frames = np.stack(normalized_frames, 0) #/ 255.0\n",
    "\n",
    "    # Stack frames to create a tensor with shape [num_frames, height, width]\n",
    "    tensor_frames = np.stack(normalized_frames)\n",
    "\n",
    "    # Add a channel dimension to the tensor\n",
    "    tensor_frames = np.expand_dims(tensor_frames, axis=1)\n",
    "    print(tensor_frames.shape)\n",
    "\n",
    "    # Convert frames to tensor\n",
    "    tensor_frames = torch.tensor(tensor_frames)\n",
    "\n",
    "    return tensor_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1ef53b4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:40:09.067906Z",
     "start_time": "2023-05-26T09:40:09.066306Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "#\n",
    "# def plot_frames(frames_to_plot):\n",
    "#     num_frames = frames_to_plot.shape[0]\n",
    "#     for i in range(num_frames):\n",
    "#         frame = frames_to_plot[i]  # Extract the frame\n",
    "#         if frame.ndim == 3:  # If the frame is 3D, reshape it to 2D\n",
    "#             frame = frame.squeeze()\n",
    "#         plt.imshow(frame, cmap='gray')\n",
    "#         plt.axis('off')\n",
    "#         plt.show()\n",
    "\n",
    "def plot_frames(frames_to_plot):\n",
    "    num_frames = frames_to_plot.shape[0]\n",
    "    num_cols = 4  # Number of columns in the grid\n",
    "    num_rows = (num_frames + num_cols - 1) // num_cols  # Number of rows in the grid\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    for i in range(1, num_frames + 1):\n",
    "        frame = frames_to_plot[i-1]\n",
    "        if frame.ndim == 3:\n",
    "            frame = frame.squeeze()\n",
    "\n",
    "        ax = fig.add_subplot(num_rows, num_cols, i)\n",
    "        ax.imshow(frame, cmap='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def predict_classes(model, input_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        _, predicted_classes = torch.max(probabilities, 1)\n",
    "        class_percentages = [(idx, p.item() * 100) for idx, p in enumerate(probabilities[0])]\n",
    "        # for idx, percentage in class_percentages:\n",
    "        #     print(f\"Class {idx}: {percentage:.2f}%\")\n",
    "        return predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6ec32e56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:40:09.077482Z",
     "start_time": "2023-05-26T09:40:09.071570Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a98bc765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-26T09:40:18.557300Z",
     "start_time": "2023-05-26T09:40:09.083523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded params/tot params:149/151\n",
      "miss matched params: ['gru.weight_ih_l0', 'gru.weight_ih_l0_reverse']\n",
      "length 56\n",
      "(56, 1, 88, 88)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (56, 1, 88, 88) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 124\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[0;32m--> 124\u001b[0m \u001b[43mpredict_all_custom_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlrw-border-se-mixup-label-smooth-cosine-lr-wd-1e-4-acc-0.88460.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[161], line 74\u001b[0m, in \u001b[0;36mpredict_all_custom_videos\u001b[0;34m(weights_file)\u001b[0m\n\u001b[1;32m     72\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, file_name)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 74\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     predictions_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m actual \u001b[38;5;241m==\u001b[39m predicted:\n",
      "Cell \u001b[0;32mIn[161], line 42\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(filename, weights_file)\u001b[0m\n\u001b[1;32m     39\u001b[0m numpy_frames \u001b[38;5;241m=\u001b[39m tensor_frames\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     41\u001b[0m frames \u001b[38;5;241m=\u001b[39m frames\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mplot_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m predict_classes(video_model, frames)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Pass the frames through the model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[159], line 26\u001b[0m, in \u001b[0;36mplot_frames\u001b[0;34m(frames_to_plot)\u001b[0m\n\u001b[1;32m     23\u001b[0m         frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     25\u001b[0m     ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(num_rows, num_cols, i)\n\u001b[0;32m---> 26\u001b[0m     \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     ax\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/__init__.py:1442\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1444\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1445\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1446\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_axes.py:5665\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5658\u001b[0m im \u001b[38;5;241m=\u001b[39m mimage\u001b[38;5;241m.\u001b[39mAxesImage(\u001b[38;5;28mself\u001b[39m, cmap\u001b[38;5;241m=\u001b[39mcmap, norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[1;32m   5659\u001b[0m                       interpolation\u001b[38;5;241m=\u001b[39minterpolation, origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[1;32m   5660\u001b[0m                       extent\u001b[38;5;241m=\u001b[39mextent, filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[1;32m   5661\u001b[0m                       filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m   5662\u001b[0m                       interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5663\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 5665\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5666\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5668\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/image.py:710\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A[:, :, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (56, 1, 88, 88) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "from model.model import VideoModel\n",
    "from utils.helpers import load_missing\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = []\n",
    "with open('label_sorted.txt') as myfile:\n",
    "    labels = myfile.read().splitlines()\n",
    "\n",
    "# Define the desired input shape for the video model\n",
    "input_shape = (88, 88)  # Adjust the dimensions according to the model's requirements\n",
    "\n",
    "\n",
    "# video_model = VideoModel(500)\n",
    "# weight = torch.load(\"/tf/weights/lrw-cosine-lr-acc-0.85080.pt\", map_location=torch.device('cpu'))\n",
    "# load_missing(video_model, weight.get('video_model'))\n",
    "# video_model.eval()\n",
    "\n",
    "# Replace 'filename.mp4' with the path to your video file\n",
    "# filename = '/tf/single-videos/FOUND_00002.mp4'\n",
    "\n",
    "def predict(filename, weights_file):\n",
    "    video_model = VideoModel(500)\n",
    "    weight = torch.load(weights_file, map_location=torch.device('cpu'))\n",
    "    load_missing(video_model, weight.get('video_model'))\n",
    "    video_model.eval()\n",
    "\n",
    "    # Get the video frames\n",
    "    raw_frames = extract_opencv(filename)\n",
    "    # plot_frames(torch.tensor(frames))\n",
    "    # Preprocess the frames\n",
    "    frames = preprocess_frames(raw_frames)\n",
    "\n",
    "    # Access the preprocessed frames tensor\n",
    "    tensor_frames = frames.squeeze(0)\n",
    "\n",
    "    # Convert the tensor frames to numpy array\n",
    "    numpy_frames = tensor_frames.numpy()\n",
    "\n",
    "    plot_frames(numpy_frames)\n",
    "\n",
    "    frames = frames.unsqueeze(0)\n",
    "    predict_classes(video_model, frames)\n",
    "\n",
    "    # Pass the frames through the model\n",
    "    with torch.no_grad():\n",
    "        predictions = video_model(frames)\n",
    "         # print(f\"predictions tensor: {predictions}\")\n",
    "    # Get the predicted label\n",
    "    predicted_label = torch.argmax(predictions)\n",
    "    print(predicted_label)\n",
    "\n",
    "    predicted_label = predicted_label.item()\n",
    "\n",
    "    # Print the predicted label\n",
    "    print(\"file\", filename)\n",
    "    print(f'Predicted label: {predicted_label}')\n",
    "    print(f'Prediction: {labels[predicted_label]}')\n",
    "\n",
    "    return labels[predicted_label]\n",
    "\n",
    "def predict_all_custom_videos(weights_file):\n",
    "    folder_path = \"/tf/single-videos/custom\"\n",
    "    weights_path = \"/tf/weights\"\n",
    "    weight_path = os.path.join(weights_path, weights_file)\n",
    "\n",
    "    predictions_count = 0\n",
    "    correct_predictions = 0\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        actual = file_name.split(\"_\")[0]\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if file_path.endswith(\".mp4\"):\n",
    "            predicted = predict(file_path, weight_path)\n",
    "            predictions_count += 1\n",
    "\n",
    "            if actual == predicted:\n",
    "                correct_predictions += 1\n",
    "\n",
    "            print(f\"correct {correct_predictions}/{predictions_count}\")\n",
    "\n",
    "    if predictions_count > 0:\n",
    "        print(\"correct:\", correct_predictions)\n",
    "        print(\"total:\", correct_predictions)\n",
    "        print(\"acc:\", (correct_predictions/predictions_count)*100.0)\n",
    "\n",
    "def weights_experiment():\n",
    "    results = {}\n",
    "    videos_path = \"/tf/single-videos/custom\"\n",
    "    weights_path = \"/tf/weights\"\n",
    "\n",
    "    predictions_count = 0\n",
    "    correct_predictions = 0\n",
    "    for weights_file in os.listdir(weights_path):\n",
    "        print(\"testing\", weights_file)\n",
    "        for file_name in os.listdir(videos_path):\n",
    "            actual = file_name.split(\"_\")[0]\n",
    "            file_path = os.path.join(videos_path, file_name)\n",
    "            weight_path = os.path.join(weights_path, weights_file)\n",
    "\n",
    "            if file_path.endswith(\".mp4\"):\n",
    "                predicted = predict(file_path, weight_path)\n",
    "                predictions_count += 1\n",
    "\n",
    "                if actual == predicted:\n",
    "                    correct_predictions += 1\n",
    "\n",
    "                print(f\"correct {correct_predictions}/{predictions_count}\")\n",
    "\n",
    "        if predictions_count > 0:\n",
    "            print(\"weights\", weights_file)\n",
    "            print(\"correct:\", correct_predictions)\n",
    "            print(\"total:\", correct_predictions)\n",
    "\n",
    "            acc = (correct_predictions/predictions_count)*100.0\n",
    "            print(\"acc:\", acc)\n",
    "\n",
    "            results[weights_file] = acc\n",
    "            print(results)\n",
    "\n",
    "\n",
    "    print(results)\n",
    "\n",
    "predict_all_custom_videos(\"lrw-border-se-mixup-label-smooth-cosine-lr-wd-1e-4-acc-0.88460.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7e076",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-26T09:40:18.556801Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76cf877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
