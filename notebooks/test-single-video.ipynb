{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfeb927e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'loqui-refactor'\n",
      "/tf\n",
      "fatal: not a git repository (or any parent up to mount point /)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "fatal: not a git repository (or any parent up to mount point /)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "\u001B[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.1.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "%cd loqui\n",
    "! git checkout prepare-all-videos\n",
    "! git pull origin test-model-single-video\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42aa6153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: requirements.txt: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b990ebf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01maugmenter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m center_crop\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from utils.augmenter import center_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def preprocess_frames(frames):\n",
    "    processed_frames = []\n",
    "\n",
    "    for frame in frames:\n",
    "        # Decode the frame data\n",
    "        img = cv2.imdecode(np.frombuffer(frame, np.uint8), -1)\n",
    "\n",
    "        # Resize the frame\n",
    "        resized_frame = cv2.resize(img, input_shape)\n",
    "\n",
    "        # Convert frame to grayscale\n",
    "        grayscale_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Normalize the frame\n",
    "        normalized_frame = grayscale_frame.astype(np.float32) / 255.0\n",
    "\n",
    "        # Add frame to the list of processed frames\n",
    "        processed_frames.append(normalized_frame)\n",
    "\n",
    "    # Stack frames to create a tensor with shape [num_frames, height, width]\n",
    "    tensor_frames = np.stack(processed_frames)\n",
    "\n",
    "    # Add a channel dimension to the tensor\n",
    "    tensor_frames = np.expand_dims(tensor_frames, axis=1)\n",
    "\n",
    "    # Convert frames to tensor\n",
    "    tensor_frames = torch.tensor(tensor_frames)\n",
    "\n",
    "    return tensor_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43cb584",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jpeg = TurboJPEG()\n",
    "\n",
    "def extract_opencv(file_name: str) -> list:\n",
    "    \"\"\"\n",
    "    Gets a path to a video file and tries to extract the ROI from it.\n",
    "    :param file_name: Path to the video file.\n",
    "    :return: ROI of the given video file.\n",
    "    \"\"\"\n",
    "\n",
    "    video = []\n",
    "    cap = cv2.VideoCapture(file_name)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()  # BGR\n",
    "        if ret:\n",
    "            roi = frame[115:211, 79:175]\n",
    "            video.append(roi)\n",
    "        else:\n",
    "            break\n",
    "    cap.release()\n",
    "\n",
    "    return video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef53b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frames(frames_to_plot):\n",
    "    num_frames = frames_to_plot.shape[0]\n",
    "    for i in range(num_frames):\n",
    "        frame = numpy_frames[i]  # Extract the frame\n",
    "        if frame.ndim == 3:  # If the frame is 3D, reshape it to 2D\n",
    "            frame = frame.squeeze()\n",
    "        plt.imshow(frame, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9193fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc54e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd learn-an-effective-lip-reading-model-without-pains\n",
    "!git pull origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f042bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_missing(model, pretrained_dict):\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict.keys() and v.size() == model_dict[k].size()}                \n",
    "    missed_params = [k for k, v in model_dict.items() if not k in pretrained_dict.keys()]\n",
    "    \n",
    "    print('loaded params/tot params:{}/{}'.format(len(pretrained_dict),len(model_dict)))\n",
    "    print('miss matched params:',missed_params)\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5070d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class args_to_send():\n",
    "    n_class = 5\n",
    "    se = False\n",
    "    border = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec32e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "!ls\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98bc765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from model.model import VideoModel\n",
    "import torchvision.transforms as transforms\n",
    "from turbojpeg import TurboJPEG\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define the desired input shape for the video model\n",
    "input_shape = (88, 88)  # Adjust the dimensions according to the model's requirements\n",
    "\n",
    "\n",
    "video_model = VideoModel(args_to_send)\n",
    "weight = torch.load(\"/tf/loqui/checkpoints/lrw-aviad/_iter_19559_epoch_3_v_acc_0.52400_.pt\", map_location=torch.device('cpu'))\n",
    "load_missing(video_model, weight.get('video_model'))\n",
    "video_model.eval()\n",
    "\n",
    "# Replace 'filename.mp4' with the path to your video file\n",
    "filename = '/tf/single-videos/ACTION_01002.mp4'\n",
    "\n",
    "# Get the video frames\n",
    "frames = extract_opencv(filename)\n",
    "\n",
    "# Preprocess the frames\n",
    "frames = preprocess_frames(frames)\n",
    "\n",
    "# Access the preprocessed frames tensor\n",
    "tensor_frames = frames.squeeze(0)\n",
    "\n",
    "# Convert the tensor frames to numpy array\n",
    "numpy_frames = tensor_frames.numpy()\n",
    "\n",
    "plot_frames(numpy_frames)\n",
    "\n",
    "\n",
    "# Pass the frames through the model\n",
    "with torch.no_grad():\n",
    "    frames = frames.unsqueeze(0)\n",
    "    predictions = video_model(frames)\n",
    "    print(f\"predictions: {predictions}\")\n",
    "# Get the predicted label\n",
    "predicted_label = torch.argmax(predictions)\n",
    "print(predicted_label)\n",
    "\n",
    "predicted_label = predicted_label.item()\n",
    "\n",
    "# Print the predicted label\n",
    "print(f'Predicted label: {predicted_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1000, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
